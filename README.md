
# **Procesamiento de datos en una infraestructura cloud**

### **EVIDENCIA DE APRENDISAJE 1, 2 y 3**

**INTEGRANTES:**

**OSCAR LUIS MARQUEZ ARRIETA**

**NATALIA JADITH CASTRO OSPINO**

**PREICA2502B020061**

## **PROBLEMATICA**

La empresa de fluido electrico EnergyCol especializada en el servicio de energia electrica a gran parte de las regiones de colombia, tanto residenciales como comerciales. Esta presentando problemas con el procesamineto de los datos de todos los clientes debido al alto tama√±o de informacion que se tienen de todos los clientes.

Debido al a esta problematica, se implementara una base de datos bien estructurada y organiza, que permita implementar toda la informacion que se requiera de los usuarios. Esto le brindara a la empresa una mejor organizacion y las relaciones entre el tama√±o del edificio, la ocupaci√≥n, la ubicaci√≥n geogr√°fica y los costos de energ√≠a.

## **Dataset**

**Fuente:** Kaggle: Residential and Commercial Energy Cost Dataset

**Enlace:** https://www.kaggle.com/datasets/andreylss/residential-and-commercial-energy-cost-dataset

# **VARIABLES RELEVANTES**

**id_cliente:** Identificador √∫nico para cada cliente

**tipo_cliente:** Tipo de propiedad (residencial o comercial)

**costo_energ√≠a_brl:** Costo mensual de energ√≠a en moneda local

**tipo_cliente:** Tipo de propiedad (residencial o comercial)

**regiones:** Regi√≥n geogr√°fica (Norte, Noreste, Medio Oeste, Sudeste, Sur)

üß∞ **Tecnolog√≠as Utilizadas**

üöÄ **Databricks Community Edition**

- Plataforma unificada para an√°lisis y procesamiento de datos

- Entorno gestionado para ejecutar SQL, PySpark y Delta Lake

- Incluye:

- Databricks Runtime (seg√∫n configuraci√≥n del cl√∫ster)

- Notebooks colaborativos

- DBFS y Volumes como sistema de almacenamiento

üî• **Apache Spark**

- Motor distribuido para procesamiento de datos a gran escala

- Utilizado mediante

- Spark SQL para consultas, DDL y validaci√≥n

- DataFrames de PySpark para an√°lisis y profiling

- Optimizaci√≥n mediante Catalyst Optimizer

üíæ **Delta Lake**

- Formato de almacenamiento transaccional

- Beneficios aplicados

- Transacciones ACID

- Versionado de datos

- Lectura y escritura eficiente

- Compatibilidad completa con SQL en Databricks

üßÆ **SQL (Spark SQL)**

- Utilizado para

- Creaci√≥n de tablas con CREATE TABLE USING DELTA

- Inserci√≥n de datos mediante INSERT INTO

- Validaci√≥n estructural con DESCRIBE TABLE y SHOW CREATE TABLE

- Consultas anal√≠ticas con GROUP BY y funciones agregadas

üêç **PySpark**

- Utilizado para validaciones t√©cnicas

- df.printSchema()

- df.describe().show()

- groupBy con agregaciones

- Lectura del archivo CSV con esquema inferido o definido

- Exploraci√≥n del dataset mediante DataFrames

üìÇ **DBFS ‚Äì Databricks File System**

- Sistema de almacenamiento utilizado en el proyecto

- Se us√≥ para

- Guardar el archivo CSV subido manualmente

- Almacenar las tablas Delta generadas

- Ruta utilizada

- /Volumes/workspace/bigdata/semana3/energy_consumption.csv

üìå **Resumen General del Proyecto**

- Se implement√≥ un pipeline completo de big data en Databricks

- Se carg√≥ el dataset original en formato CSV hacia DBFS / Volumes

- Se dise√±√≥ y cre√≥ un modelo normalizado compuesto por tablas Delta

- Se llenaron las tablas mediante SQL usando una vista temporal del CSV

- Se realizaron validaciones en SQL y PySpark para asegurar calidad de datos

- Se document√≥ la infraestructura, carga, persistencia y consultas del proyecto
